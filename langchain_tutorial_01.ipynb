{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랭체인(langchain)의 OpenAI GPT 모델(ChatOpenAI) 사용법 (1)\n",
    "##### https://teddylee777.github.io/langchain/langchain-tutorial-01/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경설정\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모듈 설치(openai, langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai 파이썬 패키지 설치\n",
    "#pip install openai langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2024-04-07일 기준 설치된 openai, langchain 확인\n",
    "```\n",
    "% pip list | grep openai\n",
    "langchain-openai           0.0.8\n",
    "openai                     1.14.3 \n",
    "\n",
    "% pip list | grep langchain\n",
    "langchain                  0.1.12\n",
    "langchain-community        0.0.28\n",
    "langchain-core             0.1.32\n",
    "langchain-experimental     0.0.54\n",
    "langchain-openai           0.0.8\n",
    "langchain-text-splitters   0.0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage-002\n",
      "dall-e-2\n",
      "dall-e-3\n",
      "davinci-002\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo-0301\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-16k-0613\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4\n",
      "gpt-4-0125-preview\n",
      "gpt-4-0613\n",
      "gpt-4-1106-preview\n",
      "gpt-4-1106-vision-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-4-vision-preview\n",
      "text-embedding-3-large\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n",
      "tts-1\n",
      "tts-1-1106\n",
      "tts-1-hd\n",
      "tts-1-hd-1106\n",
      "whisper-1\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# 사용 가능한 모델 리스트 출력\n",
    "#model_list = sorted([m['id'] for m in client.models.list()['data']]) # return 데이터 구조가 변경됨.\n",
    "model_list = sorted([m.id for m in client.models.list().data])\n",
    "for m in model_list:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatOpenAI\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI 사의 채팅 전용 Large Language Model(llm)\n",
    "- temperature : 사용할 샘플링 온도는 0과 2 사이에서 선택합니다. 0.8과 같은 높은 값은 출력을 더 무작위하게 만들고, 0.2와 같은 낮은 값은 출력을 더 집중되고 결정론적으로 만듭니다.\n",
    "- max_tokens : 채팅 완성에서 생성할 토큰의 최대 개수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "# langchain==0.2.0부터 langchain에서 가져오기는 더 이상 지원되지 않습니다. 대신 langchain-community에서 가져옵니다.\n",
    "# from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "#`langchain_community.chat_models.openai.ChatOpenAI` 클래스는 langchain-community 0.0.10에서 더 이상 사용되지 않으며 0.2.0에서 제거될 예정입니다. \n",
    "# 클래스의 업데이트된 버전이 langchain-openai 패키지에 있으므로 대신 사용해야 합니다. \n",
    "# 이를 사용하려면 `pip install -U langchain-openai`를 실행하고 `from langchain_openai import ChatOpenAI`로 가져옵니다.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[답변]: content='대한민국의 수도는 서울이야.' response_metadata={'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(temperature=0,               # 창의성 (0.0 ~ 2.0) \n",
    "                 max_tokens=2048,             # 최대 토큰수\n",
    "                 model_name='gpt-3.5-turbo',  # 모델명\n",
    "                )\n",
    "\n",
    "# 질의내용\n",
    "question = '대한민국의 수도는 뭐야?'\n",
    "\n",
    "# 질의\n",
    "# the function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
    "#print(f'[답변]: {llm.predict(question)}')\n",
    "\n",
    "print(f'[답변]: {llm.invoke(question)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "# https://python.langchain.com/docs/changelog/core/\n",
    "# BaseChatModel methods __call__, call_as_llm, predict, predict_messages. Will be removed in 0.2.0. Use BaseChatModel.invoke instead.\n",
    "# BaseChatModel methods apredict, apredict_messages. Will be removed in 0.2.0. Use BaseChatModel.ainvoke instead.\n",
    "# BaseLLM methods __call__, predict, predict_messages. Will be removed in 0.2.0. Use BaseLLM.invoke` instead.\n",
    "# BaseLLM methods apredict, apredict_messages. Will be removed in 0.2.0. Use BaseLLM.ainvoke instead.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 프롬프트 템플릿의 활용\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `PromptTemplate` : 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\n",
    "##### 사용법\n",
    "- template : 템플릿 문자열입니다. 이 문자열 내에서 중괄호 {}는 변수를 나타냅니다.\n",
    "- input_variables : 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\n",
    "##### `input_variables` : input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다.\n",
    "##### 사용법\n",
    "- 리스트 형식으로 변수 이름을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 질문 템플릿 형식 정의\n",
    "template = '{country}의 수도는 뭐야?'\n",
    "\n",
    "# 템플릿 완성\n",
    "prompt = PromptTemplate(template=template, input_variables=['country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLMChain 객체\n",
    "##### `LLMChain` : LLMChain은 특정 PromptTemplate와 연결된 체인 객체를 생성합니다\n",
    "##### 사용법\n",
    "- `prompt` : 앞서 정의한 PromptTemplate 객체를 사용합니다.\n",
    "- `llm` : 언어 모델을 나타내며, 이 예시에서는 이미 어딘가에서 정의된 것으로 보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연결된 체인(Chain)객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': '일본', 'text': '일본의 수도는 도쿄입니다.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(\"3 -> {}\".format(llm_chain.invoke(\"일본\")))\n",
    "llm_chain.invoke(\"일본\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': '캐나다', 'text': '캐나다의 수도는 오타와(Ottawa)입니다.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke({'country': '캐나다'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "호주의 수도는 캔버라입니다.\n",
      "중국의 수도는 베이징(北京)입니다.\n",
      "네덜란드의 수도는 암스테르담입니다.\n"
     ]
    }
   ],
   "source": [
    "input_list = [\n",
    "    {'country': '호주'},\n",
    "    {'country': '중국'},\n",
    "    {'country': '네덜란드'}\n",
    "]\n",
    "\n",
    "result = []\n",
    "# input_list 에 대한 결과 반환\n",
    "result = llm_chain.apply(input_list)\n",
    "\n",
    "# 반복문으로 결과 출력\n",
    "for res in result:\n",
    "    print(res['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text='호주의 수도는 캔버라입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='호주의 수도는 캔버라입니다.', response_metadata={'finish_reason': 'stop', 'logprobs': None}))], [ChatGeneration(text='중국의 수도는 베이징(北京)입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='중국의 수도는 베이징(北京)입니다.', response_metadata={'finish_reason': 'stop', 'logprobs': None}))], [ChatGeneration(text='네덜란드의 수도는 암스테르담입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='네덜란드의 수도는 암스테르담입니다.', response_metadata={'finish_reason': 'stop', 'logprobs': None}))]] llm_output={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 58, 'total_tokens': 111}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8'} run=[RunInfo(run_id=UUID('632dcbf4-5915-4b4b-b7fc-6a5b5782a0bc')), RunInfo(run_id=UUID('2ff3756b-477a-4b38-ad7d-a60a8f6b3114')), RunInfo(run_id=UUID('f04f63e7-e905-4743-a15d-1ffb46111533'))]\n"
     ]
    }
   ],
   "source": [
    "# input_list 에 대한 결과 반환\n",
    "generated_result = llm_chain.generate(input_list)\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 53,\n",
       "  'prompt_tokens': 58,\n",
       "  'total_tokens': 111},\n",
       " 'model_name': 'gpt-3.5-turbo',\n",
       " 'system_fingerprint': 'fp_b28b39ffa8'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 사용량 출력\n",
    "generated_result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RunInfo(run_id=UUID('632dcbf4-5915-4b4b-b7fc-6a5b5782a0bc')),\n",
       " RunInfo(run_id=UUID('2ff3756b-477a-4b38-ad7d-a60a8f6b3114')),\n",
       " RunInfo(run_id=UUID('f04f63e7-e905-4743-a15d-1ffb46111533'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run ID 출력\n",
    "generated_result.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "호주의 수도는 캔버라입니다.\n",
      "중국의 수도는 베이징(北京)입니다.\n",
      "네덜란드의 수도는 암스테르담입니다.\n"
     ]
    }
   ],
   "source": [
    "# 답변 출력\n",
    "for gen in generated_result.generations:\n",
    "    print(gen[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'area1': '서울', 'area2': '파리', 'text': '서울과 파리의 시차는 8시간입니다. 서울은 GMT+9 시간대에 속하고, 파리는 GMT+1 시간대에 속하기 때문에 시차가 8시간이 발생합니다.'}\n"
     ]
    }
   ],
   "source": [
    "# 질문 템플릿 형식 정의\n",
    "template = '{area1} 와 {area2} 의 시차는 몇시간이야?'\n",
    "\n",
    "# 템플릿 완성\n",
    "prompt = PromptTemplate(template=template, input_variables=['area1', 'area2'])\n",
    "\n",
    "# 연결된 체인(Chain)객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# 체인 실행: run() \n",
    "#print(llm_chain.run(area1='서울', area2='파리'))\n",
    "print(llm_chain.invoke({'area1': '서울', 'area2': '파리'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파리와 뉴욕의 시차는 6시간입니다. 파리는 그리니치 평균시(GMT+1)를 따르고, 뉴욕은 동부 표준시(EST 또는 GMT-5)를 따르기 때문에 시차가 발생합니다.\n",
      "서울과 하와이의 시차는 19시간입니다. 서울은 GMT+9 시간대에 속해 있고, 하와이는 GMT-10 시간대에 속해 있기 때문입니다.\n",
      "켄버라와 베이징의 시차는 2시간입니다. 켄버라는 GMT+10 시간대에 속하고, 베이징은 GMT+8 시간대에 속하기 때문입니다.\n"
     ]
    }
   ],
   "source": [
    "input_list = [\n",
    "    {'area1': '파리', 'area2': '뉴욕'},\n",
    "    {'area1': '서울', 'area2': '하와이'},\n",
    "    {'area1': '켄버라', 'area2': '베이징'}\n",
    "]\n",
    "\n",
    "# 반복문으로 결과 출력\n",
    "result = llm_chain.apply(input_list)\n",
    "\n",
    "for res in result:\n",
    "    print(res['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대한민국의 수도는 서울이야.content='대한민국의 수도는 서울이야.' response_metadata={'finish_reason': 'stop'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(temperature=0,               # 창의성 (0.0 ~ 2.0) \n",
    "                 max_tokens=2048,             # 최대 토큰수\n",
    "                 model_name='gpt-3.5-turbo',  # 모델명\n",
    "                 streaming=True,              \n",
    "                 callbacks=[StreamingStdOutCallbackHandler()]\n",
    "                )\n",
    "\n",
    "# 질의내용\n",
    "question = '대한민국의 수도는 뭐야?'\n",
    "\n",
    "# 스트리밍으로 답변 출력\n",
    "#response = llm.predict(question)\n",
    "response = llm.invoke(question)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gabriel",
   "language": "python",
   "name": "gabriel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
